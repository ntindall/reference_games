{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import re;\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pyl\n",
    "import nltk as nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "%matplotlib inline\n",
    "#enable longer display\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate messages with tangram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_msgs_raw = (pd.read_csv('../../data/tangrams/message/tangramsMessages.csv', escapechar='\\\\')\n",
    "              .assign(tangramRef = 'None'))\n",
    "d_boards = (pd.read_csv('reformattedBoards.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag with super simple, conservative heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious strategy is to (on a first pass) assume that the tangram the matcher moves in response to a message is the one the message is referring to. The second pass is to skip the ones where we know they got it wrong. We'll probably end up hand-tagging those or using some other strategy depending on how many there are.\n",
    "\n",
    "There are a few obvious problems here:\n",
    "\n",
    "1. The director will sometimes send several messages before the matcher moves anything. So we can't just use the closest move in time... \n",
    "2. instead, we could use the *first* move action after the message and then rule it out so that we won't use it again even if it's the first after later message as well\n",
    "3. **that**, though, also has a problem. Multiple messages are sent per tangram, and some messages are meta-chatter (e.g. \"hello\", \"thanks\", \"good job\", \"this HIT is terrible\"). If we assign the drop actions to the first $N$ messages, we'll have a bunch of actual messages about tangrams that aren't tagged and a bunch of messages **not** about tangrams incorrectly tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... we'll do a simpler thing. Check for numbers occuring in the text and look them up in the board data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('[\\W_]+')\n",
    "for index, row in d_msgs_raw.iterrows():\n",
    "    stripedStr = pattern.sub(' ', row.contents)\n",
    "    numbers = [int(s) for s in stripedStr.split() if s.isdigit()]\n",
    "    gameid = row.gameid\n",
    "    roundNum = row.roundNum\n",
    "    if len(numbers) == 1 and 0 < numbers[0] <= 12 and row.sender == 'director':\n",
    "        boardRow = d_boards.query('gameid == \"{0}\" and roundNum == {1} and trueLoc == {2}'\n",
    "                                  .format(gameid, roundNum, numbers[0]))\n",
    "        d_msgs_raw.set_value(index, 'tangramRef', boardRow.tangramName.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how many we tagged..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 - Counter(d_msgs_raw['tangramRef'])['None'] / float(d_msgs_raw.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not bad for a conservative heuristic! Now we're going to use the tagged data to train a classifier that will make predictions for the other 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set up training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used `d_msgs_raw` in `d_combined` the first time and subsequently used the updated hand-tagged version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "d_handtagged = pd.read_csv('handTagged.csv')\n",
    "d_nicki = (pd.read_csv('../../data/tangrams/old/oldTangrams.csv')\n",
    "    .query('tangram != \"*\"')\n",
    "    .drop('sender', 1)\n",
    "    .rename(columns = {'tangram' : 'tangramRef'}))\n",
    "d_combined = (d_handtagged # d_msgs_raw\n",
    "  .query('tangramRef != \"None\"')\n",
    "  .query('tangramRef != \"*\"')\n",
    "  .drop('sender', 1)\n",
    "  .append(pd.DataFrame(data = d_nicki), ignore_index=True))\n",
    "train_msg, test_msg = train_test_split(d_combined, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largely drawn from [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).\n",
    "\n",
    "Import necessary sklearn modules and grid search params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)],\n",
    "              'vect__stop_words': (None, 'english'),\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3, 1e-4, 1e-5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train bag-of-words LR classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='log', penalty='l2',n_iter=5)),\n",
    "                    ])\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf\n",
    "_ = gs_clf.fit(train_msg.contents, train_msg.tangramRef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at performance on held-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = gs_clf.predict(test_msg.contents)\n",
    "correct = predicted == test_msg.tangramRef\n",
    "print(\"test-split accuracy is...\")\n",
    "print(sum(correct)/float(len(correct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_msg.loc[:, 'predicted'] = predicted\n",
    "test_msg.loc[:, 'correct'] = test_msg['predicted'] == test_msg['tangramRef']\n",
    "test_msg.loc[:, 'maxProb'] = [max(row) for row in gs_clf.predict_proba(test_msg['contents'])]\n",
    "# We could also measure confidence using the distance between the top two categories, but this\n",
    "# turns out not to be quite as good a metric\n",
    "test_msg.loc[:, 'probDiff'] = [sorted(row)[-1] - sorted(row)[-2] \n",
    "                               for row in gs_clf.predict_proba(test_msg['contents'])]\n",
    "\n",
    "actualNumPos= float(sum(test_msg['correct']))\n",
    "actualNumNeg= len(test_msg['correct']) - float(sum(test_msg['correct']))\n",
    "\n",
    "TPRs, FPRs, thresholds = [], [], []\n",
    "for threshold in np.arange(0,1,.05) :\n",
    "    thresholds.append(threshold)\n",
    "    # Get the ones that our policy tags as \"correct\"\n",
    "    predYes = test_msg.query('maxProb > {0}'.format(threshold))['correct']\n",
    "    # TPR: number *correct* positive results relative to overall number positive samples \n",
    "    TPRs.append(sum(predYes)/actualNumPos)\n",
    "    # TPR: number *incorrect* positive results relative to overall number negative samples \n",
    "    FPRs.append((len(predYes)-sum(predYes))/actualNumNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect = 'equal')\n",
    "ax.plot([0,1], [0,1])\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.plot(FPRs, TPRs, label = 'maxProb') \n",
    "\n",
    "cautiousThreshold = [threshold for threshold, FPR in zip(thresholds, FPRs) if FPR < 0.05 ][0]\n",
    "print(cautiousThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are best params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure()\n",
    "cm = metrics.confusion_matrix(test_msg.tangramRef, predicted)\n",
    "tangramLabels = sorted(list(set(test_msg.tangramRef)))\n",
    "plot_confusion_matrix(cm, tangramLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag full dataset using ROC threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_myData = gs_clf.predict(d_handtagged.contents)\n",
    "maxProbs = [max(row) for row in gs_clf.predict_proba(d_handtagged.contents)]\n",
    "existingTags = d_handtagged.tangramRef\n",
    "autoTags = [prediction if maxProb > cautiousThreshold and existing == 'None' else existing\n",
    "            for (existing, maxProb, prediction) \n",
    "            in zip(existingTags, maxProbs, predicted_myData)]\n",
    "print(sum(autoTags != existingTags))\n",
    "d_handtagged.loc[:, 'autoTags'] = autoTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_handtagged.drop('tangramRef', axis = 1).to_csv(\"autoTagged.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('handTagged.csv')\n",
    "d_nicki = (pd.read_csv('../../data/tangrams/old/oldTangrams.csv')\n",
    "    .query('tangram != \"*\"')\n",
    "    .drop('sender', 1)\n",
    "    .rename(columns = {'tangram' : 'tangramRef'}))\n",
    "\n",
    "# Drop time column\n",
    "d = (d_raw\n",
    "    .copy()\n",
    "    .drop('time', 1)\n",
    "    .query('tangramRef != \"0\"')\n",
    "    .query('tangramRef != \"None\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 1: Generate file for POS analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Stanford CoreNLP server\n",
    "\n",
    "Before running this notebook, [get CoreNLP](http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip), go into its directory, and run\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000`\n",
    "\n",
    "If you're using port 9000 for something else, change that value and then change `PORT` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PORT = 9000\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:{}'.format(PORT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: follow Will's advice to parse unicode..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stanford_pos(text):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "       CoreNLP handles all tokenizing, at the sentence and word level.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples (str, str)\n",
    "       The first member of each pair is the word, the second its POS tag.          \n",
    "    \"\"\"\n",
    "    try:\n",
    "        ann = nlp.annotate(\n",
    "            text, \n",
    "            properties={'annotators': 'pos', \n",
    "                        'outputFormat': 'json'})\n",
    "        lemmas = []\n",
    "        for sentence in ann['sentences']:\n",
    "            for token in sentence['tokens']:\n",
    "                lemmas.append((token['word'], token['pos']))\n",
    "    except Exception as e:\n",
    "        print(text + \": cannot parse\")\n",
    "        lemmas = []\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "def is_comp_sup(word, pos, tags, check_lemmatizer=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    word, pos : str, str\n",
    "        The lemma.\n",
    "    \n",
    "    tags : iterable of str\n",
    "        The tags considered positive evidence for comp/sup morphology.\n",
    "       \n",
    "       \n",
    "    check_lemmatizer : bool\n",
    "        If True, then if the `pos` is in `tags`, we also check that\n",
    "        `word` is different from the lemmatized version of word\n",
    "        according to WordNet, treating it as an adjective. This \n",
    "        could be used to achieve greater precision, perhaps at the\n",
    "        expense of recall.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    bool       \n",
    "    \"\"\"\n",
    "    if pos not in tags:\n",
    "        return False\n",
    "    if check_lemmatizer and LEMMATIZER.lemmatize(word, 'a') == word:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_noun(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'NN', 'NNS', 'NNP', 'NNPS'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_prep(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'IN'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_verb(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_det(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'DT', 'WDT'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_pronoun(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'PRP', 'PRP$', 'WP', 'WP$'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_adjective(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'JJ', 'JJR', 'JJS'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_adverb(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'RB', 'RBR', 'RBS', 'RP', 'WRB'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_num(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'CD'}, check_lemmatizer=check_lemmatizer)\n",
    "\n",
    "def is_other(word, pos, check_lemmatizer=False):\n",
    "    return is_comp_sup(\n",
    "        word, pos, {'EX', 'FW', 'LS', 'PDT', 'POS', 'SYM', 'TO', 'UH'}, check_lemmatizer=check_lemmatizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A lemma is a (word, pos) tag pair.\n",
    "d['lemmas'] = [stanford_pos(text) for text in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d['tokens'] = [[element[0] for element in l] for l in d['lemmas']]\n",
    "d['pos'] = [[element[1] for element in l] for l in d['lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['numWords'] = [pd.value_counts(words).sum() for words in d['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts for each POS label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['nouns'] = [sum([1 if is_noun(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['prepositions'] = [sum([1 if is_prep(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['verbs'] = [sum([1 if is_verb(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['determiners'] = [sum([1 if is_det(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['pronouns'] = [sum([1 if is_pronoun(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['adjectives'] = [sum([1 if is_adjective(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['adverbs'] = [sum([1 if is_adverb(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['numbers'] = [sum([1 if is_num(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['others'] = [sum([1 if is_other(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to csv for plotting in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(d.drop([\"lemmas\", \"contents\", \"tokens\"], 1)\n",
    " .to_csv(\"posTagged.csv\", index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 2: Calculate indicator words for tangrams/rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get list of words in first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "d_round1 = d[d['roundNum'] == 1]\n",
    "\n",
    "# Pull out all tokens and collapse into count dict\n",
    "tokenDict = Counter([item for sublist in d_round1['tokens'].tolist()\n",
    "                     for item in sublist])\n",
    "\n",
    "# Pull out all words that occur more than once\n",
    "wordList = [word for (word,count) in tokenDict.items() if count > 1 and not word.isdigit()]\n",
    "print(wordList[0:10])\n",
    "print(len(wordList))\n",
    "\n",
    "# Get POS map; will be longer because it doesn't require count > 1, but it doesn't matter\n",
    "POSdict = {word: POS for lemma in d_round1['lemmas'] for (word, POS) in lemma}\n",
    "print(len(POSdict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all game ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "print(gameidList[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all tangram names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "print(tangramList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to select words & counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getWordCounts(df, gameid, roundNum, tangram = None) :\n",
    "    roundCond = 'roundNum == ' + roundNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'tangramRef == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['tokens'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "#creates mini dataframe that grabs the words used in round n for a given tangram and gameid\n",
    "def selectTangramRoundWords(df, tangram, roundNum, gameid):\n",
    "    wordCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "    return list(wordCounts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to compute PMIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that merging is really costly -- if we need to speed it up, this might be the first target. Can also vectorize the log operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns a table with the all words above 0 PMI and their counts for a given tangram\n",
    "#calculate the probability for words given tangram A ------ p(x|y)\n",
    "def makeMyPMI(df, tangram, roundNum, gameid, totals):\n",
    "\n",
    "    # count words w/in tangram\n",
    "    tangramCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "\n",
    "    #total number of words \n",
    "    tangramNumWords = (1 if sum(tangramCounts.values()) == 0 \n",
    "                       else sum(tangramCounts.values()))\n",
    "\n",
    "    #dataframe to compare \n",
    "    indicatorDF = pd.merge(pd.DataFrame(list(tangramCounts.items()), columns=['word', 'count']),\n",
    "                           pd.DataFrame(list(totals[\"counts\"].items()), columns=['word', 'totalCount']),\n",
    "                           on='word', how = 'inner')\n",
    "\n",
    "    #calculate PMI without log first. Having trouble with float issues. \n",
    "    indicatorDF['roughPMI'] = ((indicatorDF['count']/tangramNumWords)\n",
    "                                / (indicatorDF['totalCount']/totals[\"numWords\"]))\n",
    "    indicatorDF['logPMI'] = [math.log10(num) for num in indicatorDF['roughPMI']]\n",
    "    \n",
    "    #remove column rough PMI\n",
    "    indicatorDF = indicatorDF.drop('roughPMI', 1)\n",
    "    \n",
    "    return indicatorDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out PMIs & matching rates for all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do a sloppy optimization by only computing total counts once and only when necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memoize(d, gameid, counts) : \n",
    "    if \"counts\" not in counts : \n",
    "        counts[\"counts\"] = getWordCounts(d, gameid, \"1\")\n",
    "        counts[\"numWords\"] = float(sum(counts[\"counts\"].values()))\n",
    "        return counts\n",
    "    else :\n",
    "        return counts\n",
    "\n",
    "with open('matchAndPMI.csv', 'a', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['word', 'POS', 'match', 'pmi', 'total'])\n",
    "    for word in wordList :\n",
    "        print(word + \":\" + POSdict[word])\n",
    "        pmi = 0\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for gameid in gameidList:  \n",
    "            memoizedCounts = {}\n",
    "            for tangram in tangramList:\n",
    "                memoizedCounts = memoize(d, gameid, memoizedCounts)\n",
    "                round1WordList = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "                total = total + 1 if word in round1WordList else total\n",
    "                if word in round1WordList :\n",
    "                    PMI_df = makeMyPMI(d, tangram, \"1\", gameid, memoizedCounts)\n",
    "                    pmi = pmi + PMI_df[PMI_df['word'] == word]['logPMI'].tolist()[0]\n",
    "                    round6WordList = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                    match = (match + 1 if (word in round1WordList and word in round6WordList)\n",
    "                             else match)\n",
    "        writer.writerow([word, POSdict[word], float(match) / float(total), pmi/total, total])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bootstrap analysis (might want to move to R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: exclude numbers earlier in the pipeline, \n",
    "\n",
    "TODO: don't average over matches and pmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grab words with highestPMI for a given tangram/gameid\n",
    "def highestPMIWords(d, tangram, roundNum, gameid):\n",
    "    allTangramCounts = {}\n",
    "    allTangramCounts['counts'] = getWordCounts(d, gameid, \"1\")\n",
    "    allTangramCounts['numWords'] = float(sum(allTangramCounts[\"counts\"].values()))\n",
    "\n",
    "    PMIdf = (makeMyPMI(d, tangram, roundNum, gameid, allTangramCounts))\n",
    "\n",
    "    # Remove numbers\n",
    "    PMIdf['POS'] = [POSdict[word] for word in PMIdf['word']]\n",
    "    PMIdf = PMIdf.query('POS != \"CD\"'.format())\n",
    "\n",
    "    #if PMIdf has words, pull out max values, it is empty return it as is\n",
    "    if len(PMIdf.index) > 0:\n",
    "        PMI_values = PMIdf.logPMI.unique()\n",
    "        maxPMI = PMI_values.max()\n",
    "        PMIdf = PMIdf.loc[PMIdf['logPMI'] == maxPMI]\n",
    "        PMIdfword = PMIdf['word']\n",
    "        return PMIdfword.tolist()\n",
    "    else: \n",
    "        return PMIdf\n",
    "\n",
    "with open('PMIbootstrap.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['sampleNum', 'tangram', 'gameid', 'numCandidates', 'match', 'highest'])\n",
    "    for gameid in gameidList :\n",
    "        for tangram in tangramList :\n",
    "            round1Words = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "            if len(round1Words) > 0:\n",
    "                # First, write highest PMI match\n",
    "                highPMIWords = highestPMIWords(d, tangram, \"1\", gameid)\n",
    "                round6Words = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                match = np.mean([1 if word in round6Words else 0 for word in highPMIWords ])\n",
    "                writer.writerow([0, tangram, gameid, len(highPMIWords), match, \"highest\"])\n",
    "\n",
    "                # Next, take a bunch of null samples\n",
    "                for i in range(numSamples) :\n",
    "                    randomWord = np.random.choice(round1Words)\n",
    "                    match = np.mean([1 if randomWord in round6Words else 0])\n",
    "                    writer.writerow([i + 1, tangram, gameid, 1, match, \"null\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
