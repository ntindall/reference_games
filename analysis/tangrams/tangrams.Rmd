---
title: "tangramsReference"
output: html_document
---

# Import data

```{r}
library(ggplot2)
library(lmer)
library(lmerTest)
library(tidyr)
library(dplyr)
library(qdap)
library(stringr)
library(knitr)
library(tm)
library(NLP)
library(readr)
setwd("~/Repos/reference_games/analysis")
```

We've already done most of the work using nltk in the ipython notebook, so here we just read in the csv files we created there, do a bit of post-processing, and make plots.

# Replication: words over time

```{r}
#tangramMsgs = read_csv("../data/tangrams/message/tangramsMessages.csv") %>%
tangramMsgs = read_csv("tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

tangramSubjInfo = read.csv("../data/tangrams/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  filter(!is.na(nativeEnglish)) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

mean(tangramSubjInfo$totalLength / 1000 / 60)
```

```{r}
tangramCombined <- tangramMsgs %>%
  left_join(tangramSubjInfo, by = c("gameid", "role")) %>%
  filter(nativeEnglish != "no") %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

length(unique(tangramCombined$gameid))
```

```{r}
# TODO: bootstrap CIs
pdf("tangramsFigs/wordOverTime.pdf")
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words (by director) per figure") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw() 
dev.off()
```

# Additional Result 1: Parts of speech 

What are most common POS?

```{r}
d = read.csv('tangrams/posTagged.csv', header =T) %>%
  filter(sender == "director") %>%
  group_by(roundNum) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(nouns)/sum(numWords),
            numbers = sum(numbers)/sum(numWords),
            verbs = sum(verbs)/sum(numWords),
            dets= sum(determiners)/sum(numWords),
            pronouns = sum(pronouns)/sum(numWords),
            preps = sum(prepositions)/sum(numWords),
            adjectives = sum(adjectives)/sum(numWords),
            adverbs = sum(adverbs)/sum(numWords)) %>%
  mutate(OTHER = (1 - nouns - verbs - dets - pronouns -
                      preps - adjectives - adverbs - numbers)) %>%
  gather(POS, prop, nouns:OTHER) %>%
  select(roundNum, POS, prop) 
  
head(d)

ggplot(d, aes(x = roundNum, y = prop, fill = POS)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  theme_bw()
```

Same thing but make them add up to number of words so we can also see that decrease over time:
TODO: figure out why there's an uptick on the last couple rounds for some of the tangrams...
TODO: Fix the ones where they mention all of em in one row

```{r}
d = tangramCombined  %>% 
  inner_join(read.csv('tangrams/posTagged.csv', header =T)) %>%
  filter(sender == "director") %>%
  group_by(roundNum, tangramRef) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  mutate(other = (numWords - nouns - verbs - dets - pronouns -numbers-
                      preps - adjectives - adverbs)) %>%
  gather(POS, total, nouns:other) %>%
  select(roundNum, tangramRef, POS, total) 

head(d)
ggplot(d, aes(x = roundNum, y = total, fill = POS)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  #facet_wrap(~ tangramRef) +
  theme_bw()

```

# Additional Result 2: PMI

Scatter plot:

```{r}
distinctiveness_d <- read.csv("tangrams/matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences > 1) %>%
  #filter(POS == "NN") %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

summary(lm(match ~ pmi, data = distinctiveness_d))

ggplot(distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_smooth(method = 'lm') +
  theme_bw() +
  scale_colour_manual(values=cbbPalette)+
  guides(color=FALSE)
```

Look at pmi across POS...

```{r}
pos_d <- read.csv("tangrams/matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  group_by(POS) %>%
  summarize(se = sd(pmi)/sqrt(length(pmi)),
            mean_pmi = mean(pmi),
            num = sum(total),
            mean_match = mean(match)) %>%
  filter(num > 200)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(pos_d, aes(x = reorder(POS,mean_pmi,
                     function(x)-x), y = mean_pmi)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(ymax = mean_pmi + se, ymin = mean_pmi - se)) +
  theme_bw() +
  xlab("part of speech") +
  ylab("pointwise mutual information")

```


Alternatively, can do a nonparametric analysis: draw a random word from each tangram/gameid pair and look at the percentage that match with round 6... This gives a null distribution. Then we can take the highest PMI word (or words) for each tangram/gameid pair and look at the percentage of *those* that match. We see that it's much higher than expected under the null.

```{r}
# TODO: get red to show up in legend
nonparametric_d = read.csv("tangrams/PMIbootstrap.csv", header = TRUE) %>%
  mutate(PMI = factor(highest, levels = c('null', 'highest'), labels = c('random', 'top')))

highestValAvg = nonparametric_d %>% filter(highest == 'highest') %>% summarize(avg = mean(match))

nonparametric_d %>%
  group_by(sampleNum, PMI) %>%
  filter(PMI == 'random') %>%
  summarize(avgMatchRate = mean(match)) %>%
  ungroup() %>%
  ggplot(aes(x = avgMatchRate, fill = PMI)) +
  geom_histogram(binwidth = .0075) +
  geom_vline(aes(xintercept = highestValAvg), 
             color = 'red', linetype = "dashed", size = 2) +
  xlab("probability of match b/w Rounds 1 & 6") +
  scale_fill_manual(values = c("random" = "black", "top"= "red")) +
  theme_bw() 
  #guides(color=FALSE)
```

# Additional Result 3: Arbitrariness

## Part A: wordclouds

```{r}
library(wordcloud)   

oldGrams = read.csv("tangrams/handTagged.csv", quote = '"') %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

textPerGram = oldGrams %>%
  group_by(gameid, tangramRef) %>%
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(roundNum == 6) %>%
  summarize(a = paste(cleanMsg, collapse = " ")) %>%
  group_by(tangramRef) %>%
  summarize(text = paste(a, collapse = " ")) %>%
  rename(docs = tangramRef) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  png(paste("wordcloudForTangram", i, ".png", sep = ""), bg = "transparent")
  freq <- sort(colSums(as.matrix(dtm[i,])), decreasing=TRUE)
  # print(entropy(freq))
   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
  dev.off()
}
```

## Part B: across-pair entropy and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != "None") %>%
  filter(tangramRef != "*") %>%
  group_by(gameid, tangramRef) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangramRef) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(tangramRef != '*') %>%
  group_by(tangramRef, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangramRef") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangramRef)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```

### Supplemental: accuracy over time

Bring boards into it?

```{r}
tangramBoards = read.csv("../data/tangrams/finalBoard/tangramsFinalBoards.csv") %>%
  gather(tangram, location, subA:trueL) %>% 
  separate(tangram, into = c("type", "tangramName"), -2) %>% 
  spread(key = type, value = location)  %>%
  rename(matcherLoc = sub, trueLoc = true) %>%
  mutate(match = matcherLoc == trueLoc)
write.csv(tangramBoards, "reformattedBoards.csv", row.names = F)
```

subject-level performance over time?

```{r}
tangramBoards %>% 
  group_by(gameid, roundNum) %>% 
  summarize(matchProp = sum(match)/12) %>%
  ggplot(aes(x = roundNum, y = matchProp, color = gameid)) +
    geom_line()
```

histogram of overall accuracy

```{r}
tangramBoards %>% 
  group_by(gameid) %>% 
  summarize(matchProp = sum(match)/(6 * 12)) %>%
  ggplot(aes(x = matchProp)) +
    geom_histogram()
```

histogram of accuracy for each round

```{r}
tangramBoards %>% 
  group_by(gameid, roundNum) %>% 
  summarize(matchProp = sum(match)/(12)) %>%
  ggplot(aes(x = matchProp)) +
    geom_histogram() +
  facet_wrap(~ roundNum)
```

average performance over time (participants get better)

```{r}
tangramBoards %>% 
  group_by(gameid, roundNum) %>% 
  summarize(matchProp = sum(match)/(12)) %>%
  filter(matchProp > .25) %>%
  group_by(roundNum) %>%
  summarize(m = mean(matchProp), se = sd(matchProp)/sqrt(length(matchProp))) %>%
  ggplot(aes(x = roundNum, y = m)) +
    geom_line() +
    geom_errorbar(aes(ymax = m + se, ymin = m - se) )
```

