///fold:
var _powerset = function(set) {
  if (set.length == 0)
    return [[]];
  else {
    var rest = _powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

var powerset = function(set, opts) {
  var res = _powerset(set);
  return opts.noNull ? filter(function(x){return !_.isEmpty(x);}, res) : res;
};

var cartesianProductOf = function(listOfLists) {
  return reduce(function(b, a) { 
    return _.flatten(map(function(x) {     
      return map(function(y) {             
        return x.concat([y]);                   
      }, b);                                       
    }, a), true);                                  
  }, [ [] ], listOfLists);                                   
};

var nullMeaning = function(x) {return true;};
///

// possible states of the world
var states = ['t1', 't2'];
var statePrior = Categorical({vs: states, ps: [1/2, 1/2]});

// possible utterances (include null utterance to make sure dists are well-formed)
var utterances = ['the big one', 'the small one', 'n0'];
var utterancePrior = Categorical({vs: utterances, ps: [1/3,1/3,1/3]});

// longer utterances more costly
var uttCost = function(utt) {
  return utt == 'n0' ? 10 : utt.split(' ').length;
};

// meanings are possible disjunctions of states 
var meanings = map(function(l){return l.join('|');}, powerset(states, {noNull: true}));
var meaningSets = cartesianProductOf(repeat(utterances.slice(0,-1).length, function() {return meanings;}));

// Lexicons are maps from utterances to meanings (null utterance always goes to null meaning)
var lexicons = map(function(meaningSet) {
  return _.extend(_.object(utterances.slice(0,-1), meaningSet),
		  {'n0': 'null'});
}, meaningSets);
var lexiconPrior = Categorical({vs: lexicons, ps: [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9]});

// Looks up the meaning of an utterance in a lexicon object
var meaning = function(utt, lexicon) {
  var label = lexicon[utt];
  var anyMeaning = function(trueState) {
    return any(function(labelState){
      return labelState == trueState;
    },label.split('|'));
  };
  return label == 'null' ? nullMeaning : anyMeaning;
};

// set speaker optimality & noiseRate
var alpha = 3;
var noiseRate = .1;  

// Recursively delete words from an utterance with some rate
var omitWords = function(words) {
  if(_.isEmpty(words)) {
    return ['n0'];
  } else {
    var wordToOmit = words[randomInteger(words.length)];
    var remainingWords = remove(wordToOmit,words);
    return flip(noiseRate) ? omitWords(remainingWords) : words;
  }
};

// Gives distribution over possible noisy versions of utt
var noiseModel = cache(function(utt) {
  return Infer({method: 'enumerate'}, function() {
    return omitWords(utt.split(' ')).join(' ');
  });
});

// literal listener w/ noisy channel inference
var L0 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);
    var uttMeaning = meaning(intendedUtt, lexicon);
    condition(uttMeaning(state));
    observe(noiseModel(intendedUtt), utt);
    return state;
  });
});

// pragmatic speaker
// Funny thing: 'big' is as likely as 'the'
// because the prior cancels out the likelihood
// ('the' can be produced noisily from two diff utt,
// but is also half as likely to produce the correct state)
// Could we, like, collapse the noisy utterances?
var S1 = function(state, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var intendedUtt = sample(utterancePrior);
    var noisyUtt = sample(noiseModel(intendedUtt));
    factor(alpha * (L0(noisyUtt, lexicon).score(state)
		    - uttCost(noisyUtt)));
    return intendedUtt;
  });
};

// pragmatic listener (needed for S)
var L2 = function(utt, lexicon) {
  return Infer({method: 'enumerate'}, function() {
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);
    observe(noiseModel(intendedUtt), utt);    
    observe(S1(state, lexicon), intendedUtt);
    return state;
  });
};

// conventional listener
var L = function(utt, data) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var lexicon = sample(lexiconPrior);
    var intendedUtt = sample(utterancePrior);
    observe(noiseModel(intendedUtt), utt);
    observe(S1(state, lexicon), intendedUtt);
    mapData({data: data}, function(datum){
      var intendedUtt = sample(utterancePrior);      
      observe(noiseModel(intendedUtt), datum.utt);
      observe(S1(datum.obj, lexicon), intendedUtt);
    });
    return state;
  });
};

// conventional speaker
var S = function(state, data) {
  return Infer({method:"enumerate"}, function(){
    var intendedUtt = sample(utterancePrior);
    var lexicon = sample(lexiconPrior);
    var noisyUtt = sample(noiseModel(intendedUtt));
    // console.log(intendedUtt);
    // console.log(alpha * (L2(noisyUtt, lexicon).score(state) - uttCost(noisyUtt)));
    factor(alpha * (L2(noisyUtt, lexicon).score(state) - uttCost(noisyUtt)));
    mapData({data: data}, function(datum){
      observe(L2(datum.utt, lexicon), datum.obj);
    });
    return noisyUtt;
  });
};

// console.log("listener hearing small one after data:");
// console.log(L('small', [{utt: 'small', obj: 't2'}]).print());
// console.log(lexicons.length);
// map(function(lexicon) {
//   console.log(lexicon);
//   console.log(L2('the small one', lexicon));
//   console.log(L2('the big one', lexicon));  
// }, lexicons);
// console.log(L2('the small one', {'the big one' : 't1|t2', 'the small one' : 't2', 'n0' : 'null'}).print());
// console.log(L2('the big one', {'the big one' : 't1|t2', 'the small one' : 't2', 'n0' : 'null'}).print());

//console.log(S('t1', [{utt: 'the big one', obj: 't2'}, {utt: 'the small one', obj: 't1'}]).print());
console.log(S('t1', []).print());
console.log(S('t2', []).print());

// console.log("listener hearing label2 after data:");
// console.log(L('label2', [{utt: 'label1', obj: 'tangram1'}]).print());
